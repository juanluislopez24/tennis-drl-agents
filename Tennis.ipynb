{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"envs/tennis/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Actor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, nA, nS, seed):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.dense1 = nn.Linear(nS, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dense2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, nA)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.dense1.weight.data.uniform_(*hidden_init(self.dense1))\n",
    "        self.dense2.weight.data.uniform_(*hidden_init(self.dense2))\n",
    "        self.dense3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state, training=True):\n",
    "#         x = F.leaky_relu(self.bn1(self.dense1(state)))\n",
    "#         x = F.leaky_relu(self.bn2(self.dense2(x)))\n",
    "#         x = F.tanh(self.dense3(x))\n",
    "\n",
    "        x = F.leaky_relu(self.dense1(state))\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        x = F.tanh(self.dense3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Critic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, nA, nS, seed):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.dense1 = nn.Linear(nS, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dense2 = nn.Linear(nA+256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, 1)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.dense1.weight.data.uniform_(*hidden_init(self.dense1))\n",
    "        self.dense2.weight.data.uniform_(*hidden_init(self.dense2))\n",
    "        self.dense3.weight.data.uniform_(-4e-3, 4e-3)\n",
    "    \n",
    "    def forward(self, state, action, training=True):\n",
    "#         x = F.leaky_relu(self.bn1(self.dense1(state))).float()\n",
    "#         x = F.leaky_relu(self.bn2(self.dense2(torch.cat((x, action.float()), 1))))\n",
    "#         x = F.tanh(self.dense3(x))\n",
    "\n",
    "        x = F.leaky_relu(self.dense1(state))\n",
    "        x = torch.cat((x, action), 1)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Buffer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, nA, nS, seed, prioritized=False, e=0.1):\n",
    "        self.e=e\n",
    "        self.nA = nA\n",
    "        self.nS = nS\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.prioritized = prioritized\n",
    "        self.seed = random.seed(seed)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self, a=0):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.stack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.stack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define OUNoise Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  \n",
    "BATCH_SIZE = 256   \n",
    "GAMMA = 0.99       \n",
    "TAU = 2e-3      \n",
    "LR_ACTOR = 1.0e-3     \n",
    "LR_CRITIC = 1.0e-3     \n",
    "WEIGHT_DECAY = 0\n",
    "UPDATE_EVERY = 10\n",
    "UPDATE_NUM = 2\n",
    "EPSILON = 1.0\n",
    "EPS_DECAY = 1.0e-5\n",
    "EPS_END=0.02\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Class\n",
    "The difference between ddpg is when the agent learns, its critic has to take into account the states from other agents and their actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, nA, nS, nAgents, idxAgent, room, seed, prioritized=False):\n",
    "        self.prioritized = prioritized\n",
    "        self.state_size = nS\n",
    "        self.action_size = nA\n",
    "        self.idxAgent = idxAgent\n",
    "        self.room = room\n",
    "        self.nAgents = nAgents\n",
    "        \n",
    "        self.critic_local = Critic(nA*nAgents, nS*nAgents, seed).to(device)\n",
    "        self.critic_target = Critic(nA*nAgents, nS*nAgents, seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.actor_local = Actor(nA, nS, seed).to(device)\n",
    "        self.actor_target = Actor(nA, nS, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        self.t_step = 0\n",
    "        self.ouNoise = OUNoise(nA, seed)\n",
    "        \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, nA, nS, seed, prioritized=False, e=0.1)\n",
    "        self.epsilon = EPSILON\n",
    "        \n",
    "    def step(self, states, actions, rewards, next_states, dones, a=0, b=1):\n",
    "        tdE = 1.0\n",
    "#         for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "#             self.memory.add(state, action, reward, next_state, done, tdE)\n",
    "        self.memory.add(states, actions, rewards, next_states, dones)\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        \n",
    "\n",
    "        if(self.t_step == 0):\n",
    "            if(len(self.memory) > BATCH_SIZE):\n",
    "                for i in range(UPDATE_NUM):\n",
    "                    experiences = self.memory.sample(a)\n",
    "                    self.learn(experiences, GAMMA, b)\n",
    "                \n",
    "    def act(self, state, training=True):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.actor_local(state)\n",
    "        self.actor_local.train()\n",
    "        av2 =action_values.cpu().numpy()\n",
    "        if(training and (random.random() < self.epsilon)):\n",
    "            self.epsilon -= EPS_DECAY\n",
    "            if(self.epsilon < EPS_END):\n",
    "                self.epsilon = EPS_END\n",
    "            av2 += self.ouNoise.sample()\n",
    "        return(np.clip(av2, -1, 1))\n",
    "        \n",
    "    \n",
    "    def learn(self, experiences, gamma, b):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        next_actions = self.actor_target(next_states[:,self.idxAgent])\n",
    "        next_actions = self.room.getNextActionsTarget(next_states)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_Qvalues = self.critic_target(next_states.view(BATCH_SIZE,self.nAgents*self.state_size), next_actions)\n",
    "        \n",
    "#         print('original')\n",
    "#         print(next_states[:2])\n",
    "#         print('new view')\n",
    "#         print(next_states.view(BATCH_SIZE,self.nAgents*self.state_size)[:2])\n",
    "    \n",
    "#         print('next actions')\n",
    "#         print(next_actions.shape)\n",
    "#         print('nextQvalues')\n",
    "#         print(next_Qvalues.shape)\n",
    "        \n",
    "#         print('dones')\n",
    "#         print(dones.unsqueeze(1)[:,0].shape)\n",
    "#         print(dones)\n",
    "#         print('rewards')\n",
    "#         print(rewards.unsqueeze(1).shape)\n",
    "        \n",
    "        \n",
    "        Qtargets = rewards.unsqueeze(1) + (GAMMA*next_Qvalues * (1-dones.unsqueeze(1)[:,0]))\n",
    "#         print('Qtargets')\n",
    "#         print(Qtargets.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        Qexpected = self.critic_local(states.view(BATCH_SIZE,self.nAgents*self.state_size), actions.view(BATCH_SIZE, self.nAgents*self.action_size))\n",
    "#         print('Qexpected')\n",
    "#         print(Qexpected.shape)\n",
    "        \n",
    "        \n",
    "        loss_critic = F.mse_loss(Qexpected, Qtargets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        actions_pred = self.actor_local(states[:,self.idxAgent])\n",
    "        actions_pred = self.room.getActionsLocal(states)\n",
    "        loss_actor = -self.critic_local(states.view(BATCH_SIZE,self.nAgents*self.state_size), actions_pred).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)  \n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)  \n",
    "        \n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.ouNoise.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Controller\n",
    "I created this class to manage the multiple agents and to provide the necessary information for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentWaitingRoom():\n",
    "    def __init__(self, nAgents, nS, nA, seed=0):\n",
    "        self.nAgents = nAgents\n",
    "        self.nA = nA\n",
    "        self.agents = [Agent(nA, nS, nAgents, agent, self, seed) for agent in range(nAgents)]\n",
    "    def train(self, n_episodes=25000, max_t=2000, print_every=100):\n",
    "        scores_all = [[] for i in range(self.nAgents)]\n",
    "        scores_avg_all=[[] for i in range(self.nAgents)]\n",
    "        score_windows = [deque(maxlen=100) for i in range(self.nAgents)]\n",
    "        for i_episode in range(1, n_episodes+1):\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            for agent in self.agents:\n",
    "                agent.reset()\n",
    "            states = env_info.vector_observations\n",
    "            scores = np.zeros(self.nAgents)\n",
    "            \n",
    "            for t in range(max_t):\n",
    "                actions = [agent.act(states[agent.idxAgent]) for agent in self.agents]\n",
    "                env_info = env.step(actions)[brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                rewards = env_info.rewards\n",
    "                dones = env_info.local_done\n",
    "                scores += env_info.rewards\n",
    "                for agent in self.agents:\n",
    "                    agent.step(states, actions, env_info.rewards[agent.idxAgent], next_states, dones[agent.idxAgent])\n",
    "                states = next_states\n",
    "                if(np.any(dones)):\n",
    "                    break\n",
    "                    \n",
    "            for agent in self.agents:\n",
    "                torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_{}.pth'.format(agent.idxAgent))\n",
    "                torch.save(agent.actor_local.state_dict(), 'checkpoint_critic_{}.pth'.format(agent.idxAgent))\n",
    "                \n",
    "            message = '\\rEpisode {}'.format(i_episode)\n",
    "            for i, score in enumerate(scores):\n",
    "                score_windows[i].append(score)\n",
    "                scores_all[i].append(score)\n",
    "                scores_avg_all[i].append(round(np.mean(score_windows[i])))\n",
    "                message = message + '\\tAverage Score: '+str(i)+' '+str(round(np.mean(score_windows[i]), 3))\n",
    "                if(round(np.mean(score_windows[i]), 3) > 0.5):\n",
    "                    \n",
    "                    print('reached average 100 past episode score greater than 0.5 with agent {} at episode {}'.format(i, i_episode))\n",
    "                    return scores_all, scores_avg_all\n",
    "                \n",
    "            \n",
    "            print(message, end=\"\")\n",
    "            \n",
    "            \n",
    "                \n",
    "            if i_episode % print_every == 0:\n",
    "                print(message)\n",
    "        return scores_all, scores_avg_all\n",
    "    \n",
    "    \n",
    "    def getNextActionsTarget(self,next_states):\n",
    "        nextActions = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "#             agent.actor_target.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 next_actions = agent.actor_target(next_states[:,i])\n",
    "#                 nextActions.append(next_actions)\n",
    "#             agent.actor_target.train()\n",
    "            \n",
    "            next_actions = agent.actor_target(next_states[:,i])\n",
    "            nextActions.append(next_actions)\n",
    "            \n",
    "        return torch.stack(nextActions, dim=1, out=None).view(BATCH_SIZE, self.nA*self.nAgents)\n",
    "\n",
    "    def getActionsLocal(self, states):\n",
    "        Actions = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "#             agent.actor_local.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 actions = agent.actor_local(states[:,i])\n",
    "#                 Actions.append(actions)\n",
    "#             agent.actor_local.train()    \n",
    "            \n",
    "            actions = agent.actor_local(states[:,i])\n",
    "            Actions.append(actions)\n",
    "            \n",
    "        return torch.stack(Actions, dim=1, out=None).view(BATCH_SIZE, self.nA*self.nAgents)\n",
    "            \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "room = AgentWaitingRoom(2, state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17\tAverage Score: 0 -0.005\tAverage Score: 1 0.001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jl\\miniconda3\\envs\\mlagents\\lib\\site-packages\\ipykernel_launcher.py:92: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0 -0.004\tAverage Score: 1 -0.004\n",
      "Episode 200\tAverage Score: 0 -0.001\tAverage Score: 1 -0.005\n",
      "Episode 300\tAverage Score: 0 -0.004\tAverage Score: 1 -0.006\n",
      "Episode 400\tAverage Score: 0 -0.004\tAverage Score: 1 -0.005\n",
      "Episode 500\tAverage Score: 0 -0.005\tAverage Score: 1 -0.004\n",
      "Episode 600\tAverage Score: 0 -0.004\tAverage Score: 1 -0.004\n",
      "Episode 700\tAverage Score: 0 0.002\tAverage Score: 1 -0.0034\n",
      "Episode 800\tAverage Score: 0 0.02\tAverage Score: 1 -0.0033\n",
      "Episode 900\tAverage Score: 0 0.003\tAverage Score: 1 -0.005\n",
      "Episode 1000\tAverage Score: 0 -0.005\tAverage Score: 1 -0.005\n",
      "Episode 1100\tAverage Score: 0 -0.005\tAverage Score: 1 0.0091\n",
      "Episode 1200\tAverage Score: 0 0.021\tAverage Score: 1 0.0239\n",
      "Episode 1300\tAverage Score: 0 0.012\tAverage Score: 1 0.026\n",
      "Episode 1400\tAverage Score: 0 0.025\tAverage Score: 1 0.025\n",
      "Episode 1500\tAverage Score: 0 0.039\tAverage Score: 1 0.011\n",
      "Episode 1600\tAverage Score: 0 0.046\tAverage Score: 1 -0.006\n",
      "Episode 1700\tAverage Score: 0 0.033\tAverage Score: 1 -0.007\n",
      "Episode 1800\tAverage Score: 0 0.036\tAverage Score: 1 0.0001\n",
      "Episode 1900\tAverage Score: 0 0.052\tAverage Score: 1 0.024\n",
      "Episode 2000\tAverage Score: 0 0.055\tAverage Score: 1 0.024\n",
      "Episode 2100\tAverage Score: 0 0.065\tAverage Score: 1 0.069\n",
      "Episode 2200\tAverage Score: 0 0.082\tAverage Score: 1 0.084\n",
      "Episode 2300\tAverage Score: 0 0.073\tAverage Score: 1 0.071\n",
      "Episode 2400\tAverage Score: 0 0.068\tAverage Score: 1 0.073\n",
      "Episode 2500\tAverage Score: 0 0.11\tAverage Score: 1 0.1023\n",
      "Episode 2600\tAverage Score: 0 0.112\tAverage Score: 1 0.092\n",
      "Episode 2700\tAverage Score: 0 0.13\tAverage Score: 1 0.1324\n",
      "Episode 2800\tAverage Score: 0 0.13\tAverage Score: 1 0.1337\n",
      "Episode 2900\tAverage Score: 0 0.289\tAverage Score: 1 0.287\n",
      "Episode 3000\tAverage Score: 0 0.363\tAverage Score: 1 0.354\n",
      "Episode 3036\tAverage Score: 0 0.485\tAverage Score: 1 0.482reached average 100 past episode score greater than 0.5 with agent 0 at episode 3037\n"
     ]
    }
   ],
   "source": [
    "all_scores, all_avg_scores = room.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for score_i in all_scores:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, len(score_i)+1), score_i)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5104000077396631"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_scores[0][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4817000074312091"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_scores[1][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first agent reached a mean score of 0.51 from the last 100 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 2.600000038743019\n",
      "Score (max over agents) from episode 2: 2.400000035762787\n",
      "Score (max over agents) from episode 3: 2.4900000374764204\n",
      "Score (max over agents) from episode 4: 2.2000000327825546\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = [agent.act(states[agent.idxAgent]) for agent in room.agents]\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if(np.any(dones)):\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"envs/tennis/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.66271734 -1.5\n",
      " -0.          0.          6.04292488  5.99607611 -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "room2 = AgentWaitingRoom(2, state_size, action_size)\n",
    "\n",
    "for i, agent in enumerate(room2.agents):\n",
    "    agent.actor_local.load_state_dict(torch.load('checkpoint_actor_{}.pth'.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 2.600000038743019\n",
      "Score (max over agents) from episode 2: 2.7000000402331352\n",
      "Score (max over agents) from episode 3: 2.600000038743019\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):                                     \n",
    "    env_info = env.reset(train_mode=False)[brain_name]   \n",
    "    states = env_info.vector_observations                 \n",
    "    scores = np.zeros(num_agents)                         \n",
    "    while True:\n",
    "        actions = [agent.act(states[agent.idxAgent], False) for agent in room2.agents]\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if(np.any(dones)):\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
