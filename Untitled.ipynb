{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"envs/tennis/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, nA, nS, seed):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.dense1 = nn.Linear(nS, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dense2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, nA)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.dense1.weight.data.uniform_(*hidden_init(self.dense1))\n",
    "        self.dense2.weight.data.uniform_(*hidden_init(self.dense2))\n",
    "        self.dense3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state, training=True):\n",
    "        x = F.leaky_relu(self.bn1(self.dense1(state)))\n",
    "        x = F.leaky_relu(self.bn2(self.dense2(x)))\n",
    "        x = F.tanh(self.dense3(x))\n",
    "\n",
    "#         x = F.leaky_relu(self.dense1(state))\n",
    "#         x = F.leaky_relu(self.dense2(x))\n",
    "#         x = F.tanh(self.dense3(x))\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, nA, nS, seed):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.dense1 = nn.Linear(nS, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dense2 = nn.Linear(nA+256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, 1)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.dense1.weight.data.uniform_(*hidden_init(self.dense1))\n",
    "        self.dense2.weight.data.uniform_(*hidden_init(self.dense2))\n",
    "        self.dense3.weight.data.uniform_(-4e-3, 4e-3)\n",
    "    \n",
    "    def forward(self, state, action, training=True):\n",
    "#         x = F.leaky_relu(self.bn1(self.dense1(state))).float()\n",
    "#         x = F.leaky_relu(self.bn2(self.dense2(torch.cat((x, action.float()), 1))))\n",
    "#         x = F.tanh(self.dense3(x))\n",
    "\n",
    "        x = F.leaky_relu(self.dense1(state))\n",
    "        x = torch.cat((x, action), 1)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, nA, nS, seed, prioritized=False, e=0.1):\n",
    "        self.e=e\n",
    "        self.nA = nA\n",
    "        self.nS = nS\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.prioritized = prioritized\n",
    "        self.seed = random.seed(seed)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"tdE\"])\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done, tdE):\n",
    "        e = self.experience(state, action, reward, next_state, done, tdE)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self, a=0):\n",
    "#         PRIORITY REPLAY first attempt\n",
    "#         a = 0 if not self.prioritized\n",
    "#         experiences = random.sample(self.memory, k=self.batch_size)\n",
    "#         _, _, _, _, _, tdEs = zip(*self.memory)\n",
    "#         tdEs = np.array(list(tdEs))\n",
    "#         tdErrors = (np.abs(tdEs) + self.e)**a\n",
    "#         tdErrorsSum = np.sum(tdErrors)\n",
    "#         probabilities = tdErrors / tdErrorsSum\n",
    "#         experiencesIdx = choice(np.arange(len(tdEs)), self.batch_size, p=probabilities)\n",
    "#         experiences = [self.memory[c] for c in experiencesIdx]\n",
    "\n",
    "#         probabilities = torch.from_numpy(np.vstack(probabilities)).float().to(device)\n",
    "        probabilities = None\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.stack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.stack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones, probabilities)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  \n",
    "BATCH_SIZE = 256   \n",
    "GAMMA = 0.95      \n",
    "TAU = 2e-3      \n",
    "LR_ACTOR = 1e-3       \n",
    "LR_CRITIC = 1e-3     \n",
    "WEIGHT_DECAY = 0.0001 \n",
    "UPDATE_EVERY = 8\n",
    "UPDATE_NUM = 1\n",
    "EPSILON = 1.0\n",
    "EPS_DECAY = 2e-5\n",
    "EPS_END=0.03\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ddpgAgent():\n",
    "    def __init__(self, nA, nS, nAgents, idxAgent, room, seed, prioritized=False):\n",
    "        self.state_size = nS\n",
    "        self.action_size = nA\n",
    "        self.idxAgent = idxAgent\n",
    "        self.nAgents = nAgents\n",
    "        \n",
    "        self.critic_local = Critic(nA, nS*nAgents, seed).to(device)\n",
    "        self.critic_target = Critic(nA, nS*nAgents, seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.actor_local = Actor(nA, nS, seed).to(device)\n",
    "        self.actor_target = Actor(nA, nS, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        self.ouNoise = OUNoise(nA, seed)\n",
    "        self.epsilon = EPSILON\n",
    "        \n",
    "    def act(self, state, training=True):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(state)\n",
    "        self.actor_local.train()\n",
    "        av2 = actions.cpu().numpy()\n",
    "        if(training and (random.random()<self.epsilon)):\n",
    "            self.epsilon -= EPS_DECAY\n",
    "            if(self.epsilon < EPS_END):\n",
    "                self.epsilon = EPS_END\n",
    "            av2 += self.ouNoise.sample()\n",
    "        return(np.clip(av2, -1, 1))\n",
    "    \n",
    "    def target_act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = self.actor_target(states) + self.ouNoise.sample()\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, nAgents, nS, nA, seed=0):\n",
    "        self.nAgents=nAgents\n",
    "        self.state_size = nS\n",
    "        self.action_size = nA\n",
    "        \n",
    "        self.scores_all = None\n",
    "        self.score_windows = None\n",
    "        \n",
    "        self.agents = [Agent(nA, nS, seed)]\n",
    "        self.discount_factor = GAMMA\n",
    "        self.tau = TAU\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def update(self, states, actions, rewards, next_states, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
